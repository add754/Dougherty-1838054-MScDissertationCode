{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2b)  MSc Project 2019 Aaron Dougherty ELMo.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IEVlUoZFg1fB","colab_type":"text"},"source":["#Initial Downloads and imports"]},{"cell_type":"code","metadata":{"id":"CARvkl-nCY0S","colab_type":"code","outputId":"529f1a79-7973-4539-b707-2ca357f24bc8","executionInfo":{"status":"ok","timestamp":1565367723204,"user_tz":-60,"elapsed":8585,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":[" !pip install pydrive --upgrade\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied, skipping upgrade: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n","Requirement already satisfied, skipping upgrade: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n","Requirement already satisfied, skipping upgrade: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.10)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.6)\n","Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n","Requirement already satisfied, skipping upgrade: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (1.12.0)\n","Requirement already satisfied, skipping upgrade: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.11.3)\n","Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.6)\n","Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n","Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n","Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n","Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"daOKW2oawhZ_","colab_type":"code","outputId":"57bbe7cc-852a-46ad-8687-9d8206de0349","executionInfo":{"status":"ok","timestamp":1565367725288,"user_tz":-60,"elapsed":10562,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# !pip install yfinance --upgrade --no-cache-dir\n","import tensorflow as tf\n","from tensorflow import keras\n","print(tf.__version__)\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import matplotlib.pyplot as plt\n","\n","# import yfinance as yf \n","# print(yf.__version__)\n","\n","from datetime import datetime\n","from dateutil.parser import parse\n","\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","# import gensim\n","# import warnings\n","# warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n","# warnings.simplefilter(action='ignore', category=FutureWarning)\n","# from gensim.models.keyedvectors import KeyedVectors\n","# from gensim.models import Word2Vec\n","# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","# from gensim.models.phrases import Phrases, Phraser\n","\n","# import nltk\n","# nltk.download('punkt')\n","# from nltk import word_tokenize"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N1mgDLsNjyIo","colab_type":"code","outputId":"a1c715e4-30be-4acb-d656-f3144eca4301","executionInfo":{"status":"ok","timestamp":1565367725289,"user_tz":-60,"elapsed":8471,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iSWT7XSdLKAQ","colab_type":"code","colab":{}},"source":["import sys\n","import csv\n","\n","csv.field_size_limit(sys.maxsize)\n","\n","path = '/content/drive/My Drive/Colab Notebooks/Text_Data.csv'\n","path2 = '/content/drive/My Drive/Colab Notebooks/Emotion_Data.csv'\n","path3 = '/content/drive/My Drive/Colab Notebooks/title_content.csv'\n","final_text = pd.read_csv(path, encoding='ISO-8859-1', engine='python', error_bad_lines=False)\n","df = pd.read_csv(path2, encoding='ISO-8859-1', engine='python', error_bad_lines=False)\n","ungrouped_text = pd.read_csv(path3, encoding='ISO-8859-1', engine='python', error_bad_lines=False)\n","\n","final_text.drop('Unnamed: 0', axis=1, inplace=True)\n","df.drop('Unnamed: 0', axis=1, inplace=True)\n","ungrouped_text.drop('Unnamed: 0', axis=1, inplace=True)\n","\n","df['Date'] = final_text['Date']\n","final_text = final_text.merge(df, how='inner', on='Date')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aI1MP1aAoeia","colab_type":"text"},"source":["#ELMo"]},{"cell_type":"code","metadata":{"id":"29IZwUUmCv02","colab_type":"code","outputId":"0c126985-bd73-4830-ac25-d5fb75c6c0a2","executionInfo":{"status":"ok","timestamp":1565367743514,"user_tz":-60,"elapsed":24726,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["!pip install tensorflow\n","!pip install tensorflow-hub\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","\n","elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n","print('done')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.14.0)\n","Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.4)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n","Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n","Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.4)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n","done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p1eCbzOgdH6R","colab_type":"code","colab":{}},"source":["from tensorflow.compat.v1 import ConfigProto\n","from tensorflow.compat.v1 import InteractiveSession\n","\n","config = ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = InteractiveSession(config=config)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vIylOHNfDUZ6","colab_type":"text"},"source":["The output is a 3 dimensional tensor of shape (1, 8, 1024):\n","\n","The first dimension of this tensor represents the number of training samples. This is 1 in our case\n","The second dimension represents the maximum length of the longest string in the input list of strings. Since we have only 1 string in our input list, the size of the 2nd dimension is equal to the length of the string â€“ 8\n","The third dimension is equal to the length of the ELMo vector\n","Hence, every word in the input sentence has an ELMo vector of size 1024."]},{"cell_type":"code","metadata":{"id":"I8byPeL_DU9E","colab_type":"code","colab":{}},"source":["def elmo_vectors_doc(x):\n","  embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n","  with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","    return sess.run(tf.reduce_mean(embeddings,1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OiEHhDxuqjW","colab_type":"code","colab":{}},"source":["def elmo_vectors_word(x):\n","  embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n","  with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","    # return average of ELMo features\n","    return sess.run(embeddings)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMZ8cRA_YW0v","colab_type":"code","colab":{}},"source":["batches = [ungrouped_text[i:i+100] for i in range(0,ungrouped_text.shape[0],100)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9BLeCHoYNNx","colab_type":"code","outputId":"3bb2ec85-975d-451a-c5a4-ec763f0811ac","executionInfo":{"status":"ok","timestamp":1565342956190,"user_tz":-60,"elapsed":25702,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ungrouped_text.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(88689, 3)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"WeVxQvHWIiW-","colab_type":"code","outputId":"4c38613b-68b4-4ce9-994d-88ab48138c65","executionInfo":{"status":"ok","timestamp":1565365437750,"user_tz":-60,"elapsed":20578,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","tf.test.gpu_device_name()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"nv-J0QGWrhLw","colab_type":"code","colab":{}},"source":["# batch1 = batches[0:50]\n","# batch2 = batches[50:150]\n","# batch3 = batches[150:250]\n","# batch4 = batches[250:350]\n","# batch5 = batches[350:450]\n","# batch6 = batches[450:550]\n","# batch7 = batches[550:600]\n","# batch8 = batches[600:650]\n","# batch9 = batches[650:700]\n","# batch10 = batches[700:750]\n","# batch11 = batches[750:800]\n","# batch12 = batches[800:850]\n","batch13 = batches[850:]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8iW2T3jNDtW3","colab_type":"code","outputId":"ab68f59b-3f26-430e-a482-714c7605908e","executionInfo":{"status":"ok","timestamp":1565368000514,"user_tz":-60,"elapsed":220698,"user":{"displayName":"Aaron Dougherty","photoUrl":"","userId":"09194565351303205340"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import time\n","start = time.time()\n","\n","# Extract ELMo embeddings\n","elmo_titles_batch13 = [elmo_vectors_doc(x['title']) for x in batch13]\n","# elmo_content = [elmo_vectors(x['content']) for x in list_content]\n","# elmo_titles = [elmo_vectors(x['title']) for x in list_titlesTEST]\n","\n","done = time.time()\n","elapsed = done - start\n","print(elapsed)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["219.73166823387146\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0RNr8Tj3he7_","colab_type":"code","colab":{}},"source":["elmo_titles_batch1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"osy_1Ay3Macc","colab_type":"code","colab":{}},"source":["np.save(\"drive/My Drive/Colab Notebooks/elmo_titles_batch13\", elmo_titles_batch13)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoiT36EjF6tV","colab_type":"code","colab":{}},"source":["elmo_titles_new = np.concatenate(elmo_titles, axis = 0)\n","elmo_content_new = np.concatenate(elmo_content, axis = 0)\n","\n","print('done')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7gFVHstGTve","colab_type":"code","colab":{}},"source":["# save elmo_titles_new\n","pickle_out = open(\"elmo_titles.pickle\",\"wb\")\n","pickle.dump(elmo_titles_new, pickle_out)\n","file = drive.CreateFile({'title': 'elmo_titles.pickle'})\n","file.SetContentFile('elmo_titles.pickle')\n","file.Upload()\n","pickle_out.close()\n","\n","# save elmo_content_new\n","pickle_out = open(\"elmo_content.pickle\",\"wb\")\n","pickle.dump(elmo_content_new, pickle_out)\n","file = drive.CreateFile({'title': 'elmo_content.pickle'})\n","file.SetContentFile('elmo_content.pickle')\n","file.Upload()\n","pickle_out.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzFFdthbJ8kF","colab_type":"code","colab":{}},"source":["# load elmo_titles_new\n","pickle_in = open(\"elmo_titles.pickle\", \"rb\")\n","elmo_titles_new = pickle.load(pickle_in)\n","\n","# load elmo_content\n","pickle_in = open(\"elmo_content.pickle\", \"rb\")\n","elmo_content_new = pickle.load(pickle_in)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7QTaipCKSt-","colab_type":"code","colab":{}},"source":["elmo_titles_new.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xssVzLE0Cr_0","colab_type":"text"},"source":["#Vectorisation"]},{"cell_type":"code","metadata":{"id":"Rl48jJrsoiY3","colab_type":"code","colab":{}},"source":["def get_mean_vector(model, words):\n","  tokens = nltk.word_tokenize(words)\n","  accepted_tokens = [word for word in tokens if word in model.wv.vocab] #only accept words that are in word2vec vocabular\n","  vectors = [model[word] for word in accepted_tokens]# list word embeddings for each word in a given document\n","  if not vectors:\n","    mean_vector = np.zeros(300) # create a zero vector if the list of embeddings is empty\n","  else:\n","    mean_vector = np.mean(vectors, axis = 0) # average all embeddings in the list as a single vector\n","  return mean_vector\n","\n","  \n","def vectorize(doc_lst, model):\n","  vectors = []\n","  for string in doc_lst:\n","    vectors.append(get_mean_vector(model, string))\n","  return vectors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4TYQX63CW9G","colab_type":"code","colab":{}},"source":["title_vectors = vectorize(title_strings, w2v_model)\n","content_vectors = vectorize(content_strings, w2v_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQJFoh2xCaj_","colab_type":"code","colab":{}},"source":["ungrouped_text['title_vectors'] = pd.Series(title_vectors)\n","ungrouped_text['content_vectors'] = pd.Series(content_vectors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqnUWrrtCcl4","colab_type":"code","colab":{}},"source":["result_df = ungrouped_text.groupby('date')['title_vectors'].apply(np.mean).to_frame('Title')\n","result_df2 = ungrouped_text.groupby('date')['content_vectors'].apply(np.mean).to_frame('Content')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zljmLtTCeKt","colab_type":"code","colab":{}},"source":["result_df['Content'] = pd.Series(result_df2['Content'])\n","result_df = result_df.reset_index().rename(columns={\"date\" : \"Date\"})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xMfsz83CgPn","colab_type":"code","colab":{}},"source":["result_df.to_csv('Vectorised_Text_Data.csv')\n","!cp Vectorised_Text_Data.csv drive/My\\ Drive/Colab\\ Notebooks/"],"execution_count":0,"outputs":[]}]}