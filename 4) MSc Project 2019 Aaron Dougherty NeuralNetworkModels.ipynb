{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4) MSc Project 2019 Aaron Dougherty NeuralNetworkModels.ipynb","version":"0.3.2","provenance":[{"file_id":"1FY3Tuoz8KVNRoqkjnULGjaGLF1jD1nap","timestamp":1566131590946},{"file_id":"1Wcpset_d5yKMVEy32SrpdeaPF2v7q4nc","timestamp":1566077012230}],"collapsed_sections":["8CnNeTS0eRSh"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IEVlUoZFg1fB","colab_type":"text"},"source":["#Initial Downloads, imports and set-up"]},{"cell_type":"code","metadata":{"id":"ngr0ZUN9l5V4","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","print(tf.__version__)\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Bidirectional, TimeDistributed, Dropout\n","from keras.models import Model\n","from keras.utils import to_categorical\n","from keras.optimizers import RMSprop, Adam, Adagrad, Nadam, Adadelta, Adamax\n","from keras.backend import eval\n","import matplotlib.pyplot as plt\n","import keras.regularizers\n","from keras.regularizers import l2\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle, resample\n","\n","from scipy.stats import mode\n","\n","import sys\n","import csv\n","\n","csv.field_size_limit(sys.maxsize)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1mgDLsNjyIo","colab_type":"code","colab":{}},"source":["# Mount drive to import necessary files:\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L09_QATCExpI","colab_type":"code","colab":{}},"source":["def load_data(textfile):\n","  \"\"\" Method to load in pre created csv files which will be used to create\n","      the data\n","\n","  Parameters\n","  ----------\n","  textfile : str\n","      Name of file to load in\n","  \"\"\"\n","  # files paths\n","  path = '/content/drive/My Drive/Colab Notebooks/MSc Project 2019 Aaron Dougherty csvData/' + textfile\n","  path2 = '/content/drive/My Drive/Colab Notebooks/MSc Project 2019 Aaron Dougherty csvData/sent_fin.csv'\n","\n","  # load primary csv file containing text data\n","  df = pd.read_csv(path, encoding='ISO-8859-1', engine='python', error_bad_lines=False)\n","  # csv containing sentiment and finance data, including classification labels \n","  sent_fin = pd.read_csv(path2, encoding='ISO-8859-1', engine='python', error_bad_lines=False)\n","  # remove csv file size limit on import\n","  csv.field_size_limit(sys.maxsize)\n","  # remove useless columns\n","  df.drop('Unnamed: 0', axis=1, inplace=True)\n","  sent_fin.drop('Unnamed: 0', axis=1, inplace=True)\n","  return df, sent_fin"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"12BHSx-6Y3ld","colab_type":"text"},"source":["#Data Preparation"]},{"cell_type":"code","metadata":{"id":"Db5TXoX9GHAb","colab_type":"code","colab":{}},"source":["def prepare_data(text_df, sent_fin):\n","  \"\"\" Method to combine all data into one data frame\n","\n","  Parameters\n","  ----------\n","  text_df : pandas.DataFrame\n","      Primary data frame ontaining all text data\n","  sent_fin: pandas.DataFrame\n","      Secondary data frame containing all financial and sentiment data and labels\n","  \"\"\"\n","  # rename data frame columns\n","  text_df = text_df.rename(columns={\"date\": 'Date', 'title': 'Title',\n","                                    'content':'Content'})\n","  # combine data frames\n","  text_df = text_df.merge(sent_fin, how='inner', on='Date')\n","\n","  return text_df\n","\n","def removeNANs(df):\n","  \"\"\" Method to remove rows with no text\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","      Data frame to remove rows from\n","  \"\"\"\n","  string = ' '\n","  for index, row in df.iterrows():\n","    # check is row/column contains NAN type\n","    if type(row['Title']) is not type(string): \n","      df.drop([index], inplace=True)\n","    # check is row/column contains NAN type\n","    if type(row['Content']) is not type(string):\n","      df.drop([index], inplace=True)\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKHN17nPF4HO","colab_type":"code","colab":{}},"source":["# Preparing final set\n","df, sent_fin = load_data('title_contentSTOP.csv')\n","df = removeNANs(prepare_data(df, sent_fin))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zePlM18tcHzb","colab_type":"code","colab":{}},"source":["# check presence of class imbalance\n","minority_class = df[df.Direction == 1]\n","majority_class = df[df.Direction == 0]\n","print(len(minority_class), len(majority_class))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sCZAed1he7y","colab_type":"text"},"source":["#Training/Development/Testing Split"]},{"cell_type":"code","metadata":{"id":"nBv3Wp_L9sMh","colab_type":"code","colab":{}},"source":["def shuffle_df(df):\n","  \"\"\" Method to shuffle rows of a data frame\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","      Data frame to shuffle\n","  \"\"\"\n","  index = df.index # record data frame indexes\n","  df = shuffle(df) # ScikitLearn's shuffle method\n","  df.index = index # reset indexes\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KudhNUIiIhOd","colab_type":"code","colab":{}},"source":["# Shuffle  rows in the final data frame prior to splitting the data into\n","# training, devlopment and testing\n","df = shuffle_df(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHgkCvHAhQkc","colab_type":"code","colab":{}},"source":["# Split data into X and y variables\n","X = df.iloc[:, 0:-1]\n","y = df[['Date', 'Direction']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"giKEuL8Tj9J5","colab_type":"code","colab":{}},"source":["# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n","                                                    shuffle=False)\n","y_test.drop('Date', axis=1, inplace=True) # remove useless column from y_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0gERunD7u4i","colab_type":"code","colab":{}},"source":["# check shapes of trianing and test sets\n","print(X_train.shape, y_train.shape, '\\n', X_test.shape, y_test.shape, '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CnNeTS0eRSh","colab_type":"text"},"source":["# Swap labels (do not run unless validating an existing model)"]},{"cell_type":"code","metadata":{"id":"P2q0QLuBeWiL","colab_type":"code","colab":{}},"source":["# swap test set labels to validate a model's performance\n","y_test['Direction'] = y_test['Direction'].apply(lambda x: x+1 if x == 0 else x-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dinSc783HHe","colab_type":"text"},"source":["#Dealing with class imbalance:"]},{"cell_type":"code","metadata":{"id":"vThVyFVDBniZ","colab_type":"code","colab":{}},"source":["def recombine(X, y):\n","  \"\"\" Method to recombine the X and y\n","\n","  Parameters\n","  ----------\n","  X : pandas.DataFrame\n","      Data frame containing X variables\n","  y : pandas.DataFrame\n","      Data frame containing y variables\n","  \"\"\"\n","  train = X_train\n","  train['Direction'] = y_train['Direction'] # combine X and y\n","  return train\n","\n","def address_class_imbalance(df, minority_classification, majority_classification,\n","                            minority_target_samples, majority_target_samples):\n","  \"\"\" Method to remove class imbalance within a data set for binary classification\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","      Data frame to check for and remove class imbalances from\n","  minority_classification : int\n","      the minority class\n","  majority_classification : int\n","      the majority class\n","  minority_target_samples: int\n","      number of samples to upsample to\n","  majority_target_samples: int\n","      number of samples to downsample to\n","  \"\"\"\n","  # Split data frame in 2, 1 for each class\n","  minority_class = df[df.Direction == minority_classification]\n","  majority_class = df[df.Direction == majority_classification]\n"," \n","  # down sample majority and up sample minority the desired amount using helper\n","  # methods\n","  downsampled_majority = downsample_majority(majority_class, majority_target_samples)\n","  upsampled_minority = upsample_minority(minority_class, minority_target_samples)\n","  \n","  # combine newly sampled data frames\n","  new_df = upsampled_minority\n","  new_df = new_df.append(downsampled_majority, ignore_index=True)\n","\n","  return new_df\n","\n","# Upsample helper method\n","def upsample_minority(minority_class, target_samples):\n","  \"\"\" Method to up sample the minority class\n","\n","  Parameters\n","  ----------\n","  minority_class : int\n","      the minority class\n","  target_samples: int\n","      number of samples to upsample to\n","  \"\"\"\n","  return resample(minority_class, replace = True, n_samples = target_samples)\n","\n","# Downsample helper method\n","def downsample_majority(majority_class, target_samples):\n","  \"\"\" Method to down sample the majority class\n","\n","  Parameters\n","  ----------\n","  majority_class : int\n","      the majority class\n","  majority_target_samples: int\n","      number of samples to downsample to\n","  \"\"\"\n","  return resample(majority_class, replace = False, n_samples = target_samples)\n","\n","def find_y_mid_point(df):\n","  \"\"\" Method to find the mid-point between to class counts\n","\n","  Parameters\n","  ----------\n","  df : pandas.DataFrame\n","      dataframe to find midpoint in between 2 unbalanced classes\n","  \"\"\"\n","  # count number of rows belonging to each binary class\n","  count_class_0, count_class_1 = df.Direction.value_counts()\n","  # calculate mid-point\n","  mid_point = int(count_class_1 + ((count_class_0 - count_class_1)/2))\n","  return mid_point"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NDsi47vH_JB","colab_type":"code","colab":{}},"source":["# rebalance classes using mis point (up and down sample classes equally)\n","train = recombine(X_train, y_train) # recombine training sets\n","target_sample = find_y_mid_point(train) # determine target sample value\n","# up and down sample the classes to remove imbalance\n","train = address_class_imbalance(train, 1, 0, target_sample, target_sample)  # swap back\n","train = shuffle_df(train) # shuffle the new training set\n","# Split training set back into X and y\n","X_train = train.iloc[:, 0:-1]\n","y_train = train[['Date', 'Direction']]\n","y_train.drop('Date', axis=1, inplace=True) # remove useless column"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PtNDooBjyEVy","colab_type":"code","colab":{}},"source":["# recheck class imbalance\n","minority_class = train[train.Direction == 1]\n","majority_class = train[train.Direction == 0]\n","print(len(minority_class), len(majority_class))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1YIK6czmY_l_","colab_type":"text"},"source":["#Model Preparation"]},{"cell_type":"code","metadata":{"id":"fDltxLKoyROT","colab_type":"code","colab":{}},"source":["# define documents:\n","def create_xtext(x_df, col):\n","  \"\"\" Method to format data for the nerual network\n","\n","  Parameters\n","  ----------\n","  X_df : pandas.DataFrame\n","      X values\n","  col:  str\n","      relevant columns\n","  \"\"\"\n","  arr = []\n","  for index, row in x_df.iterrows():\n","    arr.append(x_df[col].loc[index]) # add text data to array row by row\n","  return arr\n","\n","# create vocabulary/tokeniser\n","def create_vocab(docs):\n","  \"\"\" Method to create vocabulary/dictionary\n","\n","  Parameters\n","  ----------\n","  docs : list\n","      list of strings\n","  \"\"\"\n","  tokenizer = Tokenizer() # define keras tokeniser\n","  tokenizer.fit_on_texts(docs) # create dictionary from words in docs\n","  return tokenizer\n","\n","# tokenise text\n","def tokenize(text_arr):\n","  \"\"\" Method to ftokenise text\n","\n","  Parameters\n","  ----------\n","  text_arr : list\n","      list of strings to be tokenised\n","  \"\"\"\n","  tokenised_text = tokenizer.texts_to_sequences(text_arr) # tokenise text\n","  return tokenised_text\n","\n","# find max number of words in an array of tokens\n","def find_max_length(array):\n","  \"\"\" Method to find maximum number of tokens in a list index\n","\n","  Parameters\n","  ----------\n","  array : list\n","      2D list containing tokenised text\n","  \"\"\"\n","  max_ = 0\n","  for tokens in array:\n","    if len(tokens) > max_:\n","      max_ = len(tokens)\n","  return max_\n","\n","# pad sequences to be the same length\n","def pad_text(tokens, maxlen):\n","  \"\"\" Method to pad sequence length up to max\n","\n","  Parameters\n","  ----------\n","  tokens : pandas.DataFrame\n","      list of tokenised text\n","  maxlen: int\n","      maximum lnegth of a sequence\n","  \"\"\"\n","  return pad_sequences(tokens, maxlen=maxlen, padding='post')\n","\n","# load the whole embedding into memory/create embedding index\n","def load_embeddings(path):\n","  \"\"\" Method to create an embedding index so that each word in the dictionary is\n","      matched with an embedding value\n","\n","  Parameters\n","  ----------\n","  path : str\n","      file path for where the word embeddings are stored\n","  \"\"\"\n","  embeddings_index = dict() # create empty dictionary\n","  embeddings = open(path) # open path to word embeddings\n","  for line in embeddings: # create embeddings dictinoary\n","    values = line.split() # split string on each line read in from file path\n","    word = values[0] # define first string as word\n","    # assign rest as vector and change type\n","    vector = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = vector # add word/vector pair to dictionary\n","  # confirm loading of vectors\n","  print('Loaded %s word vectors.' % len(embeddings_index))\n","  return embeddings_index\n","\n","# create a weight matrix for words\n","def create_embeddings(embeddings, vocab, vocab_size):\n","  \"\"\" Method to create an embedding matrix fro embedding ayer of neural network\n","\n","  Parameters\n","  ----------\n","  embeddings : dictionary\n","      Word embeddings dictinoary created from method above\n","  vocab : tokenizer\n","      tokenizer object containing the vocabulary\n","  vocab_size : int\n","      size of vocabulary\n","  \"\"\"\n","  # initialise mebeddings matrix with all 0s\n","  embedding_matrix = np.zeros((vocab_size, 300))\n","  for word, i in tokenizer.word_index.items(): #  build matrix using vocabulary\n","    embedding_vector = embeddings.get(word) # find vector for each word in vocab\n","    if embedding_vector is not None: # check word had a corresponding vector\n","      embedding_matrix[i] = embedding_vector # add to matrix\n","  return embedding_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bckq0XQkcTJ","colab_type":"code","colab":{}},"source":["# transform titles into array format\n","train_titles = create_xtext(X_train, 'Title')\n","\n","test_titles = create_xtext(X_test, 'Title')\n","\n","# combine all titles into complete array of documents\n","all_docs = train_titles + test_titles\n","\n","# prepare y variables (turn them into a list)\n","train_labels = y_train['Direction'].tolist()\n","test_labels = y_test['Direction'].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0g_bGYP7ePhD","colab_type":"code","colab":{}},"source":["# prepare sentiment vector inputs\n","train_sent_vecs = X_train.iloc[:, 3:10].as_matrix()\n","test_sent_vecs = X_test.iloc[:, 3:10].as_matrix()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWy6pu-Nmkn5","colab_type":"code","colab":{}},"source":["# create tokeniser/vocabulary and calculate vocab size\n","tokenizer = create_vocab(all_docs)\n","vocab_size = len(tokenizer.word_index) + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylDv7aJV38ca","colab_type":"code","colab":{}},"source":["# tokenise all text data as integers\n","tokenized_titles_train = tokenize(train_titles)\n","tokenized_titles_test = tokenize(test_titles)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IE4cDESnRAK","colab_type":"code","colab":{}},"source":["# calculate longest title in tokens\n","MAX_T_LEN = find_max_length(tokenized_titles_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uursgNHWDjOD","colab_type":"code","colab":{}},"source":["# pad documents to max length\n","padded_titles_train = pad_text(tokenized_titles_train, MAX_T_LEN)\n","padded_titles_test = pad_text(tokenized_titles_test, MAX_T_LEN)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMTbuYORgNUO","colab_type":"code","colab":{}},"source":["# load in word GLoVe word embeddings\n","path = '/content/drive/My Drive/Colab Notebooks/MSc Project 2019 Aaron Dougherty GloVe/glove.6B.300d.txt'\n","word_embeddings = load_embeddings(path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WVtKgsFtvWX","colab_type":"code","colab":{}},"source":["#create word embedding matrix\n","embedding_matrix = create_embeddings(word_embeddings, tokenizer, vocab_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4uNSpMaVzBd-","colab":{}},"source":["# convert y variables into one hot vector encodings for binary crossentropy\n","y_train__binary_matrix = to_categorical(y_train['Direction'], num_classes = 2)\n","y_test__binary_matrix = to_categorical(y_test['Direction'], num_classes = 2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-aCATHfMYUU","colab_type":"text"},"source":["#Model"]},{"cell_type":"code","metadata":{"id":"MNPRoUdqokUz","colab_type":"code","colab":{}},"source":["def create_model(vec_dim, num_classes,input_shape_title):\n","  \"\"\" Method to create a deep neural network model\n","\n","  Parameters\n","  ----------\n","  vec_dim : int\n","      Word embedding vector dimensions\n","  num_classes : int\n","      number of classes (dimensions of final layer output)\n","  input_shape_title : int\n","      shape of title array input\n","  \"\"\"\n","  # Inputs:\n","  titles_input  = Input(shape=(input_shape_title,), name='titles_input') \n","  # embedding layer\n","  embed_titles = Embedding(vocab_size, vec_dim,\n","                           weights=[embedding_matrix],\n","                           input_length=input_shape_title,\n","                           trainable=True)(titles_input)\n","  activation = 'tanh'\n","  lstm_titles  = LSTM(vec_dim, activation=activation,\n","                      input_shape=(input_shape_title, vec_dim),\n","                      return_sequences=True)(embed_titles)\n","  lstm_titles  = LSTM(vec_dim, activation=activation, return_sequences=True )(lstm_titles)\n","  lstm_titles  = LSTM(vec_dim, activation=activation )(lstm_titles)\n","  # Output/fully connected layer\n","  out = lstm_titles\n","  out = Dense(200, activation=activation)(out)\n","  out = Dense(num_classes, activation=activation)(out)\n","  # Model initialisation\n","  model = Model(inputs=[titles_input], outputs=[out])\n","  return model\n","\n","def create_bidirectional_model(vec_dim, num_classes,input_shape_title):\n","  \"\"\" Method to create a deep neural network model with bidirectional LSTMs\n","\n","  Parameters\n","  ----------\n","  vec_dim : int\n","      Word embedding vector dimensions\n","  num_classes : int\n","      number of classes (dimensions of final layer output)\n","  input_shape_title : int\n","      shape of title array input\n","  \"\"\"\n","  # Inputs:\n","  titles_input  = Input(shape=(input_shape_title,), name='titles_input') \n","  # embedding layers\n","  embed_titles = Embedding(vocab_size, vec_dim,\n","                           weights=[embedding_matrix],\n","                           input_length=input_shape_title,\n","                           trainable=True)(titles_input)\n","  activation = 'tanh' # change to relu?\n","  lstm_titles  = Bidirectional(LSTM(vec_dim, activation=activation,\n","                                    input_shape=(input_shape_title, vec_dim),\n","                                    return_sequences=True, dropout=0.2,\n","                                    recurrent_dropout=0.1))(embed_titles)\n","  lstm_titles  = Bidirectional(LSTM(vec_dim, activation=activation, return_sequences=True))(lstm_titles)\n","  lstm_titles  = LSTM(vec_dim, activation=activation)(lstm_titles)\n","  # Output/Fully connected layer\n","  out = lstm_titles\n","  out = Dense(250, activation='tanh')(out)\n","  out = Dense(200, activation='tanh')(out)\n","  out = Dense(num_classes, activation='tanh')(out)\n","  # Model initialisation\n","  model = Model(inputs=[titles_input], outputs=[out])\n","  return model\n","\n","def create_sentiment_model(vec_dim, num_classes,input_shape_title, input_shape_sentiment):\n","  \"\"\" Method to create a deep neural network model with dual input (sentiment vector)\n","\n","  Parameters\n","  ----------\n","  vec_dim : int\n","      Word embedding vector dimensions\n","  num_classes : int\n","      number of classes (dimensions of final layer output)\n","  input_shape_title : int\n","      shape of title array input\n","  input_shape_sentiment : int\n","      shape of sentiment array input\n","  \"\"\"\n","  # Inputs:\n","  titles_input  = Input(shape=(input_shape_title,), name='titles_input')\n","  sentiment_input = Input(shape=(input_shape_sentiment,),\n","                          name='sentiment_features') \n","  # embedding layers\n","  embed_titles = Embedding(vocab_size, vec_dim,\n","                           weights=[embedding_matrix],\n","                           input_length=input_shape_title,\n","                           trainable=True)(titles_input)\n","  activation = 'relu'\n","  lstm_titles  = Bidirectional(LSTM(vec_dim, activation=activation,\n","                                    input_shape=(input_shape_title, vec_dim),\n","                                    return_sequences=True))(embed_titles)\n","  lstm_titles  = Bidirectional(LSTM(vec_dim, activation=activation, return_sequences=True))(lstm_titles)\n","  lstm_titles  = Bidirectional(LSTM(vec_dim, activation=activation))(lstm_titles)\n","  # Sentiment layer\n","  sentiment = Dense(600, activation=activation)(sentiment_input)\n","  sentiment = Dense(450, activation=activation)(sentiment)\n","  sentiment = Dense(300, activation=activation)(sentiment)\n","  # Combine inputs\n","  concat = concatenate([lstm_titles, sentiment])\n","  # Output/Fully connected layer\n","  out = concat\n","  out = Dense(500, activation='tanh')(out)\n","  out = Dense(200, activation='tanh')(out)\n","  out = Dense(num_classes, activation='tanh')(out)\n","  # Model initialisation\n","  model = Model(inputs=[titles_input, sentiment_input], outputs=[out])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j2cez8BPv_zG","colab":{}},"source":["vec_dim = 300\n","num_classes = 2\n","input_shape_title = MAX_T_LEN\n","input_shape_sentiment = 7\n","\n","# instantiate model\n","model = create_bidirectional_model(vec_dim, num_classes, input_shape_title)\n","# compile the model\n","opt = Adam(lr=0.0001, clipnorm=1., decay=1e-6, amsgrad=True)\n","model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NN33Mxn49gF4","colab_type":"code","colab":{}},"source":["# summarize the model\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOpVMPsnXV-T","colab_type":"code","colab":{}},"source":["# fit the model\n","history = model.fit([padded_titles_train], y_train__binary_matrix, batch_size = 512, epochs = 5, verbose=1, validation_split=0.15)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeTu3BZf4QW-","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","print(history.history.keys())\n","# summarize history for accuracy\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kds4ed-CTPhJ","colab_type":"text"},"source":["##Save  and load model"]},{"cell_type":"code","metadata":{"id":"hDMgbe3ITpaX","colab_type":"code","colab":{}},"source":["!pip install h5py pyyaml"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"itcLmlx6Ttt3","colab_type":"code","colab":{}},"source":["# Save the weights\n","# model.save_weights('/content/drive/My Drive/Colab Notebooks/Models/')\n","# Save entire model to a HDF5 file\n","model.save('/content/drive/My Drive/Colab Notebooks/Models/model1.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfeXTWYu4iqK","colab_type":"code","colab":{}},"source":["model = keras.models.load_model('/content/drive/My Drive/Colab Notebooks/Models/model8_25epochs.h5')\n","print(keras.backend.eval(model.optimizer.lr))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dt2drdo98lMD","colab_type":"text"},"source":["##Evaluate Predictions"]},{"cell_type":"code","metadata":{"id":"xowPphqwFNe_","colab_type":"code","colab":{}},"source":["y_pred = model.predict([padded_titles_test], verbose=1)\n","y_pred_bool = np.argmax(y_pred, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3cEPssrNFbr","colab_type":"code","colab":{}},"source":["y_pred_df = pd.DataFrame(y_pred_bool)\n","y_test_df = pd.DataFrame(y_test)\n","y_pred_df['Date'] = [date for date in X_test['Date']]\n","y_test_df['Date'] = [date for date in X_test['Date']]\n","y_pred_df = y_pred_df.rename(columns={0: \"Direction\"})\n","y_pred_modes = y_pred_df.groupby('Date')['Direction'].apply(lambda x: mode(x)[0][0]).reset_index()\n","y_test_modes = y_test_df.groupby('Date')['Direction'].apply(lambda x: mode(x)[0][0]).reset_index()\n","y_pred_majority = y_pred_modes['Direction'].tolist()\n","y_test_majority = y_test_modes['Direction'].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wMD0KSDXgJo-","colab":{}},"source":["print(classification_report(y_test_majority, y_pred_majority))\n","print(confusion_matrix(y_test_majority, y_pred_majority))"],"execution_count":0,"outputs":[]}]}